{
  description = "Apache Hive is a SQL interface to tables in HDFS.";

  inputs = {
		
    nixpkgs.url = "/home/sahiti/work/nixpkgs";
    utils.url = "github:numtide/flake-utils";
  };

  outputs = { self, nixpkgs, utils }:
		utils.lib.eachSystem [ utils.lib.system.x86_64-linux ] # utils.lib.defaultSystems
			(system: rec {
				legacyPackages = import nixpkgs {inherit system;};
				defaultPackage = legacyPackages.callPackage ./default.nix { jdk = legacyPackages.jdk11; };
				checks = {
					standalone-tests = import ./test.nix {
						makeTest = import (nixpkgs + "/nixos/tests/make-test-python.nix");
						pkgs = legacyPackages;
						flake = self;
						package = defaultPackage;
					};
					
					hadoop-tests = import ./full-hadoop-test.nix {
						makeTest = import (nixpkgs + "/nixos/tests/make-test-python.nix");
						pkgs = legacyPackages;
						flake = self;
						package = defaultPackage;

					};
					
					hadoop-integration-tests = import ./test-with-hadoop.nix {
						makeTest = import (nixpkgs + "/nixos/tests/make-test-python.nix");
						pkgs = legacyPackages;
						flake = self;
						package = defaultPackage;
					};
					
					kerberos-integration-tests = import ./test-with-kerberos.nix {
						makeTest = import (nixpkgs + "/nixos/tests/make-test-python.nix");
						pkgs = legacyPackages;
						flake = self;
						package = defaultPackage;
					};
				};
				
			}) // {
				nixosModule = {config, lib, pkgs,...}:

					with lib;
					let cfg = config.services.hadoop.hiveserver;
							in
					{

						options.services.hadoop.hiveserver = {
							enable = mkEnableOption {
								default = false;
								description = "enable hiveserver";
							};
							
							hiveSite = mkOption {
								default = {};
								type = types.attrsOf types.anything;
								example = literalExpression ''
        {
          "fs.defaultFS" = "hdfs://localhost";
        }
      '';
								description = ''
        Hive configuration hive-site.xml definition
        <link xlink:href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration"/>
      '';
							};
 							hiveSiteDefault = mkOption {
								default = {
									"javax.jdo.option.ConnectionURL" = "jdbc:derby:;databaseName=/var/run/hive/metastore_db;create=true";
								};
								type = types.attrsOf types.anything;
								example = literalExpression ''
        {
          "fs.defaultFS" = "hdfs://localhost";
        }
      '';
								description = ''
        Hive configuration hive-site.xml definition
        <link xlink:href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration"/>
      '';
							};

							gatewayRole.enable = mkEnableOption "gateway role for deploying hive configs to toher nodes";
							
						};
						
						config = mkMerge [

							(mkIf config.services.hadoop.hiveserver.gatewayRole.enable {
								users.users.hive = {
									description = "hive user";
									isSystemUser = true;
									group = "hadoop";
								};
							})
							
							(mkIf cfg.enable {
								environment.systemPackages = [ self.defaultPackage.${config.nixpkgs.system} ];
								networking.firewall.allowedTCPPorts = [10000 10001 10002 14000];

								users.users.hive = {
									description = "hive user";
									isSystemUser = true;
									group = "hadoop";
								};

								services.hadoop = {
									extraConfDirs =   let
										propertyXml = name: value: lib.optionalString (value != null) ''
    <property>
      <name>${name}</name>
      <value>${builtins.toString value}</value>
    </property>
  '';
										siteXml = fileName: properties: pkgs.writeTextDir fileName ''
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <!-- generated by NixOS -->
    <configuration>
      ${builtins.concatStringsSep "\n" (pkgs.lib.mapAttrsToList propertyXml properties)}
    </configuration>
  '';
									in
										[
										(pkgs.runCommand "hive-conf" {} (with cfg; ''
mkdir -p $out/
cp ${siteXml "hive-site.xml" (hiveSiteDefault // hiveSite)}/* $out/
''))
									];
									
									gatewayRole.enable = true;
								};

								systemd.services = {
									hive-init = {
										before = [ "hiveserver.service" ];
										path = [ pkgs.hadoop ];
										serviceConfig = {

											Type = "oneshot";
											# The below are the instructions to initialize Hive resoruces given in https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHiveServer2andBeeline.
											ExecStart = with pkgs; ''
hadoop fs -mkdir       /tmp
hadoop fs -mkdir       /user/hive/warehouse
hadoop fs -chmod g+w   /tmp
hadoop fs -chmod g+w   /user/hive/warehouse
'';
											User = "hdfs";
										};
									};
									
									hiveserver =  {
										wantedBy = [ "multi-user.target" ];
										after = ["network.target" "hive-init.service" ];
										environment =
												{
													HADOOP_CONF_DIR = "/etc/hadoop-conf";
												};				
										path = [ pkgs.sudo self.defaultPackage.${config.nixpkgs.system} pkgs.coreutils ];
										serviceConfig = {
											ExecStart = ''
											sudo -u hdfs ${self.defaultPackage.${config.nixpkgs.system}}/bin/hiveserver2
						'';
											ExecStartPre = ''
mkdir /var/run/hive
'';
										};
									};
								};
							})
						];
					}
				;
			};
}
